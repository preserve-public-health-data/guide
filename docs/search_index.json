[["index.html", "Preserving Public Health Data 1 About", " Preserving Public Health Data 2025-01-30 1 About This Bookdown is a guide to preserving public health data, from planning to storage and dissemination. To navigate, click on the headings in the table of contents at the left. "],["getting-started.html", "2 Getting started 2.1 Where we are now 2.2 What we can do about it", " 2 Getting started 2.1 Where we are now 2.2 What we can do about it "],["identifying-and-locating-data.html", "3 Identifying and locating data 3.1 Subsection 1 3.2 Subsection 2", " 3 Identifying and locating data 3.1 Subsection 1 3.2 Subsection 2 "],["downloading-live-sites-and-data.html", "4 Downloading live sites and data 4.1 Determining what to download 4.2 Utilities for downloading", " 4 Downloading live sites and data 4.1 Determining what to download 4.1.1 File path reverse engineering 4.1.2 Inspecting network requests 4.2 Utilities for downloading 4.2.1 Command-line utilities 4.2.1.1 wget 4.2.1.2 curl 4.2.1.3 aria2 4.2.1.4 wget 4.2.2 R 4.2.2.1 httr 4.2.2.2 system 4.2.3 Python 4.2.3.1 Requests "],["worked-examples.html", "5 Worked examples", " 5 Worked examples "],["downloading-from-the-internet-archive.html", "6 Downloading from the Internet Archive 6.1 Using the Wayback Machine web interface 6.2 Using download scripts 6.3 Using the CDX API 6.4 Common pitfalls 6.5 Worked examples", " 6 Downloading from the Internet Archive 6.1 Using the Wayback Machine web interface 6.2 Using download scripts 6.2.1 Querying pages 6.2.2 Downloading pages 6.3 Using the CDX API If, for whatever reason, none of the above methods are available (for example on a headless server without the ability to install software), the Wayback Machine CDX API can be used in conjunction with commonly-found command-line utilities to locate files for download. The CDX API may also be useful for writing custom scripts and programs due to its minimal nature. Briefly, the CDX API allows access to the indices that the Wayback Machine uses internally to keep track of and query archived URLs. This guide will not comprehensively document the CDX API itself as the Internet Archive already maintains extensive documentation on their website. 6.3.1 Querying the CDX API The CDX API endpoint lives at http://web.archive.org/cdx/search/cdx. The main parameters of interest are: url (string): The primary filter. Must be precent-encoded. matchType (string): Describes how url is matched. The following values will be the most useful: exact: Results must match url exactly (default if matchType is not provided); prefix: Results must begin with url. Other useful parameters: from and to (integer): Filters the results by date range. Must be given in the form YYYYMMDDhhmmss; partial information is allowed (for example YYYY or YYYYMMDD). limit (integer): Limits the number of results returned. The CDX API docs note that “the CDX server may return millions or billions of record[s]”, so this may be necessary for larger or older sites. fl (one or more strings, comma-separated): Chooses which fields are returned, and in what order. The most useful fields are: timestamp: The timestamp of the capture, formatted as YYYMMDDhhmmss. original: The original URL. Not ideal for filtering due to things like port specification; see urlkey instead. mimetype: The MIME type string. urlkey and \"digest\": The standardized URL and a checksum of the content, respectively. Useful for filtering out all duplicated URLs (urlkey) or only duplicated URLs with the same content (digest). collapse (string): Filters out results by the given field, returning only the first when ordered by date. See above for fields that can be passed to collapse. filter (string formatted like [!]field:regex): Use regular expressions to filter based on field values. Most common requests will take the form of http://web.archive.org/cdx/search/cdx?url=[URL]&amp;matchType=prefix, which will return all URLs beginning with [URL]. Outputs are sorted by URL and date. 6.3.2 Reconstructing the Wayback Machine URL from CDX API output Using only the timestamp and original fields, we can reconstruct archived URLs of the results returned by the CDX API as using the following template: https://web.archive.org/web/[TIMESTAMP]/[ORIGINAL] where [TIMESTAMP] and [ORIGINAL] are the timestamp and original fields, respectively. 6.3.3 Examples Note: &amp;limit=5 has been appended to all example request URLs. Return all URLs beginning with nih.gov Request: http://web.archive.org/cdx/search/cdx?url=nih.gov&amp;matchType=host Response: gov,nih)/ 19971210191959 http://www.nih.gov:80/ text/html 200 VQYSXO37KW53LD7HTFOTW5PN5VF74CSM 1977 gov,nih)/ 19971210191959 http://www.nih.gov:80/ text/html 200 VQYSXO37KW53LD7HTFOTW5PN5VF74CSM 1977 gov,nih)/ 19971210191959 http://www.nih.gov:80/ text/html 200 VQYSXO37KW53LD7HTFOTW5PN5VF74CSM 1977 gov,nih)/ 19980204234008 http://www.nih.gov:80/ text/html 200 343MBBETTNE43WB6TJSCIIGQSPVRG6AY 1992 gov,nih)/ 19981212031409 http://www.nih.gov:80/ text/html 200 WA4UOXZMXJYHIIYABQCZB7MXJBURXNR6 2635 Return all URLs beginning with nih.gov with unique content Request: http://web.archive.org/cdx/search/cdx?url=nih.gov&amp;matchType=host&amp;collapse=digest Response: gov,nih)/ 19971210191959 http://www.nih.gov:80/ text/html 200 VQYSXO37KW53LD7HTFOTW5PN5VF74CSM 1977 gov,nih)/ 19980204234008 http://www.nih.gov:80/ text/html 200 343MBBETTNE43WB6TJSCIIGQSPVRG6AY 1992 gov,nih)/ 19981212031409 http://www.nih.gov:80/ text/html 200 WA4UOXZMXJYHIIYABQCZB7MXJBURXNR6 2635 gov,nih)/ 19990117023817 http://nih.gov:80/ text/html 200 4LFPLNNQ67M6LJT3LZBBNT3RNZKUJLXS 2604 gov,nih)/ 19990125091117 http://nih.gov:80/ text/html 200 VY6HRTZDJND4JKWNMAVH43OBEV5HKR5B 2744 Note that 3 records with duplicate content have been dropped between the previous request and this one. Return all unique URLs beginning with nih.gov Request: http://web.archive.org/cdx/search/cdx?url=nih.gov&amp;matchType=host&amp;collapse=urlkey&amp;limit=5 Response: gov,nih)/ 19971210191959 http://www.nih.gov:80/ text/html 200 VQYSXO37KW53LD7HTFOTW5PN5VF74CSM 1977 gov,nih)/!%0d%0a%20medlinep 20210505215349 http://nih.gov/!%0D%0A%20medlinep text/html 301 PAJQC5WJFC6FWJIM7WZA7WU2SW5AU43R 440 gov,nih)/!clearcommunication/plainlanguage/index.htm 20240816204904 https://www.nih.gov/!clearcommunication/plainlanguage/index.htm text/html 404 WLBW2ZNS7QAI6LLO3D6CLUYLU5SKQEXJ 8042 gov,nih)/!clearcommunication/plainlanguage/index.htm!figure 20240816203157 http://www.nih.gov/!clearcommunication/plainlanguage/index.htm!Figure unk 301 3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ 259 gov,nih)/&quot; 20000201081541 http://www.nih.gov:80/%22 text/html 404 UDSH36NBYWO2X73LNMX2LEHLNQ7FYXHZ 326 Note that this query will only return the first capture in each collapse group. To get the most recent capture for each group, it would be better to collapse by digest instead and then process the output. Return all unique PDF files (MIME type: application/pdf) beginning with nih.gov Request: http://web.archive.org/cdx/search/cdx?url=nih.gov&amp;matchType=host&amp;collapse=urlkey&amp;filter=mimetype:application/pdf Response: gov,nih)/about/97_almanac/almanac97.pdf 20041031172700 http://www.nih.gov/about/97_Almanac/almanac97.pdf application/pdf 200 B6XAXDQ5YA2BD2FC7CWH6HLG7YAGRQT5 1412860 gov,nih)/about/almanac/2001/appropriations/appropriations2001.pdf 20041107184849 http://www.nih.gov/about/almanac/2001/appropriations/appropriations2001.pdf application/pdf 200 YQNM4XRHFNFUC3RPAERP7T6F7OWS6TDQ 49644 gov,nih)/about/almanac/almanac_2006_2007.pdf 20070824135850 http://www.nih.gov/about/almanac/Almanac_2006_2007.pdf application/pdf 200 Q33VZSPQ7EUOVJ7PLGGFGLYRS4466FGV 2005821 gov,nih)/about/almanac/almanac_2008_2009.pdf 20081029135142 http://www.nih.gov/about/almanac/Almanac_2008_2009.pdf application/pdf 200 CNYNO3S7OWTPSYCJKIT2LCJTMDKZJ3WI 2610665 gov,nih)/about/almanac/archive/2001/appropriations/appropriations2001.pdf 20080924150457 http://www.nih.gov/about/almanac/archive/2001/appropriations/appropriations2001.pdf application/pdf 200 YQNM4XRHFNFUC3RPAERP7T6F7OWS6TDQ 49675 The same as the previous example, but dump the URLs only (for example to feed into a download tool) Request: http://web.archive.org/cdx/search/cdx?url=dpcpsi.nih.gov&amp;matchType=host&amp;collapse=digest&amp;filter=mimetype:application/pdf&amp;fl=original Response: http://dpcpsi.nih.gov/collaboration/2007_Report_of_Trans-NIH_Research.pdf http://dpcpsi.nih.gov/collaboration/2008_Report_of_Trans-NIH_Research.pdf http://dpcpsi.nih.gov/collaboration/2009_Report_of_Trans-NIH_Research.pdf http://dpcpsi.nih.gov/collaboration/Trans-NIH_Research.pdf http://dpcpsi.nih.gov/council/110807minutes.pdf Note that the above URLs cannot be passed to raw downloaders like wget, aria2, etc. directly if the content is no longer live and are only usable by downloaders like wayback_machine_downloader and waybackpack that are specific to the Internet Archive. See below for an example of how to reconstruct archive.org URLs that can be passed to those tools. The same as the previous example, but reconstruct the archive.org URLs using Bash (for example to feed into a download tool) curl &quot;http://web.archive.org/cdx/search/cdx?url=dpcpsi.nih.gov&amp;matchType=host&amp;collapse=digest&amp;filter=mimetype:application/pdf&amp;fl=original&quot; | while IFS=&quot; &quot; read urlkey timestamp original mimetype statuscode digest length do echo &quot;https://web.archive.org/web/${timestamp}/${original}&quot; done # From here, redirect to a file or pipe to a download tool 6.4 Common pitfalls 6.4.1 Non-nested websites and externally-linked resources Many of the common download tools and interfaces for interacting with the Internet Archive (including the CDX API) do not play well with sites whose content is not strictly nested. By this, we mean sites that utilise webpages that pull resources from: Other paths / domains of the same site, or Other sites entirely, such as content delivery networks. For example, the homepage of the NIH Sexual &amp; Gender Minority Research office, which used to live at https://dpcpsi.nih.gov/sgmro, linked resources from: https://dpcpsi.nih.gov/sites (embedded images), https://dpcpsi.nih.gov/themes (stylesheets), and https://dpcpsi.nih.gov/core (scripts), all of which are located on different directory trees and so would not show up in queries of dpcpsi.nih.gov/sgmro. This issue is more prevalent on sites that make use of content management systems like Drupal or WordPress and sites that use website building and hosting services such as those offered by Wix.com and Squarespace. The most straightforward way to work around this issue is to: Query all HTML files from the site of interest, which should hopefully be nested; Download all of the returned URLs; and Parse the the downloaded HTML files to identify tags of interest and the URLs that they link to. For example, the following bash + htmlq code will dump the URLs of images referenced by HTML files in the current directory: find . -name &#39;*.htm*&#39; -print0 | xargs -0 cat | htmlq img -a src 6.5 Worked examples "],["storage-and-dissemination.html", "7 Storage and dissemination", " 7 Storage and dissemination "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
